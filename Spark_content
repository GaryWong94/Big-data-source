Spark介绍与架构：

-------------------------------------------------------------------------------------------------------------------------

Why Spark 和与MapReduce的区别：

    MapReduce
    1. MapReduce的任务更适合一路任务，对于多路任务或需要多次迭代的任务来说，Spark更加合适。
    2. 在上一个任务完成后，都先必须把数据保存在HDFS上才能进行下一次任务。这个操作需要的磁盘I/O非常耗时。
    3. 要执行多个任务，需要对程序进行串联，导致每个任务自身都是高时延。而且必须上一个任务完成后才能开始下一个任务。

    Spark的表现
    1. RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。
       这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升比较大。

    2. Spark允许程序开发者使用有向无环图（DAG）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。

-------------------------------------------------------------------------------------------------------------------------

Spark生态圈：
   Spark生态圈以HDFS、S3为底层存储引擎，以Yarn、Mesos和Standlone作为资源调度引擎；

   基于Spark的：
   0. Spark 计算引擎 自身可以实现MapReduce应用；
   1. Spark Streaming 流式计算  可以处理实时应用
   2. MLib 机器学习库  可以实现机器学习算法
   3. GraphX 图算法  可以实现图计算
   4. Spark SQL 数据仓库  可以实现类SQL查询
   5. SparkR 实现复杂数学计算

      Spark Streaming: 将 data stream 变成 batches of data(Discretized Streams) 放入 Spark engine 中进行操作(RDD转换) 产生新的DStream
      文档： https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams
-------------------------------------------------------------------------------------------------------------------------

RDD：只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。

RDD 特点：
1. 是一个分区的只读记录的集合； 
2. 一个具有容错机制的特殊集； 
4. 可以分布在集群的节点上，以函数式操作集合的方式，进行各种并行操作

RDD  弹性理解 ：“弹性”是指在任何时候都能进行重算。
    1.自动进行内存和磁盘切换 
    2.基于lineage的高效容错 
    3.task如果失败会特定次数的重试 
    4.stage如果失败会自动进行特定次数的重试，而且只会只计算失败的partition
    5.checkpoint和persist 【内存或磁盘中对数据进行复用】
    6.数据调度弹性：DAG TASK 和资源管理无关 
    7.数据分片的高度弹性repartion
    8 RDD数据集就像块带有弹性的海绵一样，不管怎样挤压（分区遭到破坏）都是完整的。


- RDD的重要内部属性（数据结构）：

•一组partitions（分片，可扩展性）

•计算每个分片的函数（transformation，action）

•RDD间的依赖关系（自动容错）

•选择项：一个K-V RDD的partitioner（自定义分区函数，一般是使用hash函数）

•选择项：存储每个partition的优先的（preferred）位置（本地优化分配）


实际上还具有的性质：

1. immutable，意味着不能修改，只能进行转换产生另一个RDD
2. 每个数据分区的地址列表（如HDFS上的数据块的地址）

-------------------------------------------------------------------------------------------------------------------------

RDD存储结构

RDD实现的数据结构核心是一个五元组：

属性                                说明

分区列表-partitions                 每个分区为RDD的一部分数据

依赖列表-dependencies               table存储其父RDD即依赖RDD

计算函数-compute                     利用父分区计算RDD各分区的值

分区器-partitioner                   指明RDD的分区方式(hash/range)

分区位置列表-preferredLocations      指明分区优先存放的结点位置

-------------------------------------------------------------------------------------------------------------------------

partition和Executer、Task的关系： 

- 读取时的 split
1、默认情况下,当Spark从HDFS读一个文件的时候，会为一个输入的片段创建一个分区，也就是一个HDFS split对应一个RDD partition, 
   大小默认为128MB，这个过程是自动完成的，不需要人工干预，但是这个分区之间的split是基于行的分割而不是按照块分割的。

2、使用默认读取文件命令时，分区数目可能会少，一般情况下是HDFS分区数目，如果文件一行长度很长（大于block大小），分区数会变少。

- Task的数目
每个executor分配的Task的数量和executor分配的CPU数量一致，而Task数量和分区数目一致。所以要平衡分区数目和申请的CPU资源。

一般情况下，分区文件小会导致分区数目增加，可以分配到更多的节点上进行计算，这样会提升速度；分区过大则分区数目减少，如果分区数目远少于分配的CPU数目，那么很多CPU就会空闲，速度会很慢。

Spark对每个RDD分区只能运行一个并行任务，最多同时运行任务数就是集群的CPU核心总数，总体讲建议一个CPU最多可以分配2-3个任务。所以总的分区数目理想数字也应该是分配的CPU的2-3倍之间。

分区的最大大小由executor的可用内存决定，如果分区过大，多个大的分区被分配到同一个executor中，超出了shuffle内存，则会出现内存溢出。


分区任务分配

计算和数据的本地化是分布式计算的一个重要思想，当数据和运算分离的时候就需要从其他节点拉数据，这个是要消耗网络IO的。

在进行任务分配的时候，要以网络传输消耗最小化为原则，Spark从最近的节点将数据读到RDD中。
Spark首先会评估分区情况，当任务分配完毕后，会有若干个executor，而分区在若干个worker上，需要综合评估网络传输的代价，将不同的分区分配到不同的executor上。


输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。
当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。
随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。

每个节点可以起一个或多个Executor。每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。每个Task执行的结果就是生成了目标RDD的一个partiton。(由一个partition生成另一个partition)
注意:  这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。
而 Task被执行的并发度 = Executor数目 * 每个Executor核数。
至于partition的数目：对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。
在Map阶段partition数目保持不变。在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。


1. 什么是job
Job简单讲就是提交给spark的任务。当对RDD使用action时执行
2. 什么是stage
Stage是每一个job处理过程要分为的几个阶段。一般根据dependency划分
3什么是task
Task是每一个job处理过程要分几为几次任务。Task是任务运行的最小单位。最终是要以task为单位运行在executor中。
3. Job和stage和task之间有什么关系
Job----> 一个或多个stage---> 一个或多个task
5.一个stage的task的数量是有谁来决定的？
RDD的partition决定。一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据
6.一个job任务的task数量是由谁来决定的？
一个job任务可以有一个或多个stage，一个stage又可以有一个或多个task。所以一个job的task数量是  （stage数量 * task数量）的总和。 
7.每一个stage中的task最大的并行度？
并行度：是指指令并行执行的最大条数。在指令流水中，同时执行多条指令称为指令并行。
理论上：每一个stage下有多少的分区，就有多少的task，task的数量就是我们任务的最大的并行度。（一般情况下，我们一个task运行的时候，使用一个cores）
实际上：最大的并行度，取决于我们的application任务运行时使用的executor拥有的cores的数量。 
8.如果我们的task数量超过这个cores的总数怎么办？
先执行cores个数量的task，然后等待cpu资源空闲后，继续执行剩下的task。 



Partitioner决定RDD的分区方式。 
RDD的分区方式主要包含两种（HashPartitioner和RangePartitioner），这两种分区类型都是针对Key-Value类型的数据。如是非Key-Value类型，则分区为None。 
Hash是以key作为分区条件的散列分布，分区数据不连续，极端情况也可能hash到少数几个分区上，导致数据不均等；
Range按Key的排序平衡分布，分区内数据连续，大小也相对均等。

-------------------------------------------------------------------------------------------------------------------------

- RDD的创建方式: 
1. 从Hadoop文件系统（或者与Hadoop兼容的其他持久化存储系统，Hive，HBase，Cassandra）输入创建 分发list
2. 从父RDD转换得到新的RDD

-------------------------------------------------------------------------------------------------------------------------

- RDD为何高效
1. RDD不可变，而且lazy的。通过两种操作和记录compute chain进行转换。
2. 只支持粗粒度转换：
        [粗粒度： 每次操作 都作用于所有集合] 因此，对于RDD的写是粗粒度的（如map/filter/join），这种方式通过记录RDD之间的转换从而刻画RDD的继承关系，
        而不是真实的数据，最终构成一个DAG（有向无环图），而如果发生RDD丢失，RDD会有充足的信息来得知怎么从其他RDDs重新计算得到。
        读 操作 可以是粗粒度的也可以是细粒度的： 可以读其中的一条记录。
3. 通过数据的本地性来提高性能

-------------------------------------------------------------------------------------------------------------------------

- RDD的容错机制详解：
1. 使用Lineage
    记录下compute chain(lineage)，计算丢失部分就可以 
        比如：RDD1->RDD2, RDD1 partition2丢失，在RDD1->RDD3过程中，RDD1要重新从RDD0计算partition2，再计算RDD3
        
2. 使用checkpoint
    每次对RDD操作都会产生新的RDD，如果compute chain比较长，计算比较笨重，就把数据放在硬盘中
    1. Checkpoint会把当前RDD保存到一个目录中。 
    2. Checkpoint的时候，会把所有依赖的父级rdd信息清除掉。 
    3. Checkpoint不会马上执行，要触发action操作的时候才会执行。 
    4. 因为 Checkpoint会清除父级RDD的信息，所以在Checkpoint应该先做persist（持久化）操作，否则就要重新计算一遍。 
    5. 一般来说，Lineage链较长、宽依赖的RDD需要采用检查点机制。 
    6. Checkpoint的好处显而易见，比如做1000次迭代，在第999次时做了Checkpoint，如果第1000次的时候，只要重新计算第1000即可，不用从头到尾再计算一次。 
    7. 与spark提供的另一种缓存机制cache相比， cache缓存数据由executor管理，当executor消失了，被cache的数据将被清除，RDD重新计算，而checkpoint将数据保存到磁盘或HDFS，job可以从checkpoint点继续计算。
    
-------------------------------------------------------------------------------------------------------------------------


- Action 和 Transformation的区别
Transformation操作： 是指由一个RDD生成新RDD的过程，其代表了是计算的中间过程，其并不会触发真实的计算。
Action操作：         不同于Transformation操作，Action代表一次计算的结束，不再产生新的RDD，将结果返回到Driver程序。
                    所以Transformation只是建立计算关系，而Action才是实际的执行者。每个Action都会调用SparkContext的runJob方法向集群正式提交请求，所以每个Action对应一个Job。

1. 惰性求值：Action操作会触发实际的计算，而Transformation是没有触发实际计算的，是惰性求值的，从一个RDD转换生成另一个RDD的转换操作不是马上执行的，
            需要等到有Actions操作时，才开始触发运算 

2. 返回类型：Transformation操作是一个RDD转化为一个新的RDD，即返回RDD，而Action操作返回其他数据类型。

3. 输出结果：Action操作会有实际结果的输出，触发Spark提交作业（Job），并将其他数据类型输出到Spark系统。(向驱动器程序返回结果或者把结果写入外部系统)。
            Transformation并无实际输出。
            
-------------------------------------------------------------------------------------------------------------------------

NarrowDependency与ShuffleDependency
如果父RDD的每个分区最多只能被子RDD的一个分区使用，我们称之为（narrow dependency）窄依赖； 
若一个父RDD的每个分区可以被子RDD的多个分区使用，我们称之为（wide dependency）宽依赖，在源代码中方法名为ShuffleDependency，顾名思义这之中还需要Shuffle操作。 


窄依赖每个child RDD 的partition的生成操作都是可以并行的，而宽依赖则需要所有的parent partition shuffle结果得到后再进行。

NarrowDependency也还有两个子类，一个是 OneToOneDependency，一个是 RangeDependency

OneToOneDependency，可以看到getParents实现很简单，就是传进一个partitionId: Int，再把partitionId放在List里面传出去，即去parent RDD 中取与该RDD 相同 partitionID的数据

RangeDependency，用于union。与上面不同的是，这里我们要算出该位置，设某个parent RDD 从 inStart 开始的partition，逐个生成了 child RDD 从outStart 开始的partition，则计算方式为： partitionId - outStart + inStart



依赖：宽依赖(ShuffleDependency) 窄依赖(NarrowDependency)
窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用，只有map任务，不需要发生Shuffle过程
哪些操作属于窄依赖呢？

•map

•flatMap

•filter

•union


宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，需要计算好所有父RDD对应分区的数据，然后在节点之间进行Shuffle。
哪些操作属于宽依赖呢？

•groupByKey

•reduceByKey

•groupWith

•cartesian



Stage/Task
Stage
实际：   1. Stage 可以并行执行的（当两个stage不相关时，比如有了rdd1和rdd2，rdd3读map rdd1, rdd4 shuffle rdd2, 这两个可以并行执行）
        2. 存在依赖的Stage 必须在依赖的Stage执行完成后才能执行下一个Stage
        3. Stage的并行度取决于资源数
        
参考： https://www.jianshu.com/p/5fe79b67ea00


Stage可以简单理解为是由一组RDD组成的可进行优化的执行计划。如果RDD的衍生关系都是窄依赖，则可放在同一个Stage中运行，若RDD的依赖关系为宽依赖，则要划分到不同的Stage。
这样Spark在执行作业时，会按照Stage的划分, 生成一个完整的最优的执行计划。如RDD-A到RDD-B和RDD-F到RDD-G均属于宽依赖，所以与前面的父RDD划分到了不同的Stage中。 


DAG:  DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。
对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 




-------------------------------------------------------------------------------------------------------------------------
Shuffle操作：
    窄依赖是子 RDD的各个分片(partition)不依赖于其他分片,能够独立计算得到结果,宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片, 看如下两个示例:

　　Map 操作将 RDD 里的各个元素进行映射, RDD 的各个数据元素之间不存在依赖,可以在集群的各个内存中独立计算,也就是并行化。
   但是groupby 之后的 Map 操作,为了计算相同 key 下的元素个数,需要把相同 key 的元素聚集到同一个 partition 下,所以造成了数据在内存中的重新分布, 即shuffle操作. shuffle 操作是 spark 中最耗时的操作,应尽量避免不必要的 shuffle. 
　　宽依赖主要有两个过程: shuffle write 和 shuffle fetch. 类似 Hadoop 的 Map 和 Reduce 阶段.shuffle write 将 ShuffleMapTask 任务产生的中间结果缓存到内存中, shuffle fetch 通过网络获得 ShuffleMapTask 缓存的中间结果进行 ShuffleReduceTask 计算,这个过程容易造成OutOfMemory. 
　　shuffle过程内存分配使用 ShuffleMemoryManager 类管理,会针对每个 Task 分配内存,Task 任务完成后通过 Executor 释放空间.这里可以把 Task 理解成不同 key 的数据对应一个 Task. 早期的内存分配机制使用公平分配,即不同 Task 分配的内存是一样的,但是这样容易造成内存需求过多的 Task 的 OutOfMemory, 从而造成多余的 磁盘 IO 过程,影响整体的效率.
    1.5版以后 Task 共用一个内存池,内存池的大小默认为 JVM 最大运行时内存容量的16%,分配机制如下:假如有 N 个 Task,ShuffleMemoryManager 保证每个 Task 溢出之前至少可以申请到1/2N 内存,且至多申请到1/N,N 为当前活动的 shuffle Task 数,因为N 是一直变化的,所以 manager 会一直追踪 Task 数的变化,重新计算队列中的1/N 和1/2N.但是这样仍然容易造成内存需要多的 Task 任务溢出,所以最近有很多相关的研究是针对 shuffle 过程内存优化的. 

-------------------------------------------------------------------------------------------------------------------------

- 全部具体的部件和工作的整体流程


Client：客户端进程，负责提交作业到Master。

Application：Spark Application的概念和Hadoop MapReduce中的类似，指的是用户编写的Spark应用程序，包含了一个Driver 功能的代码和分布在集群中多个节点上运行的Executor代码；
            一个application通常包含三部分：从数据源取数据形成RDD，通过RDD的transformation和action进行计算，将结果输出到console或者外部存储（比方说collect收集输出到console）。

Cluster Manager：指的是在集群上获取资源的外部服务，目前有：
        Standalone：Spark原生的资源管理，由Master负责资源的分配；
        Hadoop Yarn：由YARN中的ResourceManager负责资源的分配；

Master：Standalone模式中主控节点，负责接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。

Worker：集群中任何可以运行Application代码的节点，类似于YARN中的NodeManager节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，负责管理本节点的资源，定期向 Master汇报心跳，接收Master的命令，启动Driver和Executor；

Driver： Spark中的driver感觉其实和yarn中Application Master的功能相类似。主要完成任务的调度以及和executor和cluster manager进行协调。
        带有Spark context
        一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责作业的解析、生成Stage并调度Task到Executor上。
        包括DAGScheduler，TaskScheduler。

Executor：即真正执行作业的地方，一个集群一般包含多个Executor，每个Executor接收Driver的命令Launch Task，一个Executor可以执行一到多个Task。

作业（Job）：包含多个Task组成的并行计算，往往由Spark Action产生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation；

Stage：一个Spark作业一般包含一到多个Stage。

Task：  Task是Spark中最小的执行单元。RDD一般是带有partitions的，每个partition的在一个executor上的执行任务是一个Task。 
        一个Stage包含一到多个Task，通过多个Task实现并行运行的功能。

DAGScheduler： 实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中。

TaskScheduler：实现Task分配到Executor上执行。

SparkContext：SparkContext为Spark job的入口，由Spark driver创建在client端，包括集群连接，RddID，创建抽样，累加器，广播变量等信息。

RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。

    (SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。

    SparkEnv内创建并包含如下一些重要组件的引用。

    MapOutPutTracker：负责Shuffle元信息的存储。

    BroadcastManager：负责广播变量的控制与元信息的存储。

    BlockManager：负责存储管理、创建和查找块。

    MetricsSystem：监控运行时性能指标信息。

    SparkConf：负责存储配置信息。)
    
    架构图如下：
![image](https://github.com/GaryWong94/Big-data-source/blob/master/pic/20171114093031720.jpg)



架构：

Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。
    Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；
    Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行。
    
    
Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进行控制。在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。
    Driver 程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。
    在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进行处理。


Spark的整体流程为

    standalone模式：
        1.spark集群启动后，Worker向Master注册信息
        2.spark-submit命令提交程序后，driver和application也会向Master注册信息
        3.创建SparkContext对象：主要的对象包含DAGScheduler和TaskScheduler
        4.Driver把Application信息注册给Master后，Master会根据Application信息去Worker节点启动Executor
        5.Executor内部会创建运行task的线程池，然后把启动的Executor反向注册给Dirver
        6.DAGScheduler：负责把Spark作业转换成Stage的DAG，根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；
                  同时DAGScheduler还会处理由于Shuffle数据丢失导致的失败；
        7.TaskScheduler：维护所有TaskSet，分发Task给各个节点的Executor（根据数据本地化策略分发Task），监控task的运行状态，负责重试失败的task；
        8.所有task运行完成后，SparkContext向Master注销，释放资源；
        
        
    Spark-on-yarn模式：
        yarn的组件：
        ResourceManager负责分配资源，ApplicationMaster负责应用管理，NodeManager负责计算管理
        
        1. ResourceManager：负责将集群的资源分配给各个应用使用，而资源分配和调度的基本单位是Container，其中封装了集群资源（CPU、内存、磁盘等），每个任务只能在Container中运行，并且只使用Container中的资源；
        2. NodeManager：是一个个计算节点，负责启动Application所需的Container，并监控资源的使用情况汇报给ResourceManager
        3. ApplicationMaster：主要负责向ResourceManager申请Application的资源，获取Container并跟踪这些Container的运行状态和执行进度，
            执行完后通知ResourceManager注销ApplicationMaster，ApplicationMaster也是运行在Container中；(DAGSchedule和TaskSchedule)
            
    (1)client:(driver、sparkcontext、DAGSchedule都在client中，driver和application分开)
        1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler
        2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster
        3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源
        4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver
        5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task
        6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己



    (2)cluster:(driver在application master中，driver和application在一起)
        yarn-cluster模式中，当用户向yarn提交应用程序后，yarn将分为两阶段运行该应用程序：
        第一个阶段是申请container放application master，把Spark的Dirver作为一个ApplicationMaster在container中启动；
        第二个阶段是ApplicationMaster向ResourceManager申请资源，并启动Executor来运行task，同时监控task整个运行流程并重试失败的task；




运行时的措施：

    1.在集群启动的时候，各个slave节点（也可以说是worker）会向集群的Master注册，告诉Master我随时可以干活了，随叫随到
    2.Master会根据一种心跳机制来实时监察集群中各个worker的状态，是否能正常工作
    3.Driver Application提交作业的时候也会先向Master注册信息
    4.作业注册完毕之后，Master会向worker发射Executor命令
    5.worker产生若干个Executor准备执行
    6.各个worker中的Executor会向Driver Application注册Executor信息，以便Driver Application能够将作业分发到具体的Executor
    7.Executor会定期向Driver Application报告当前的状态更新信息
    8.Driver Application发射任务到Executor执行

参考：http://www.raincent.com/content-85-10510-1.html
-------------------------------------------------------------------------------------------------------------------------

Spark调优： 
理解Shuffle对性能的影响，并尽可能减少Shuffle次数（对于Spark是Stage个数）。
理解任务并行度对性能的影响，Task并行执行符合木桶效应，利用好Spark的WebUI学会分析Task慢在哪，并设置合理的任务并发数。
对于SparkSQL注意spark.sql.shuffle.partitions参数的配置避免严重的任务倾斜，即便Task并行度设置的是合理的，也会被倾斜的任务拖慢。
避免严重的数据膨胀（类似笛卡尔积操作），防止出现OOM
利用好cache，将需要反复用到的RDD或者DataFrame进行cache，加速计算。
硬件方面：Spark进行Shuffle是要写磁盘的，给服务器配个SSD专门存Shuffle数据平均至少能带来20%的性能提升。
经常跑大任务，建议采用万兆网络环境，避免网络瓶颈。


算子方面：
reduceByKey和groupByKey的区别： 
https://www.iteblog.com/archives/1357.html

内存溢出：OOM
https://blog.csdn.net/yhb315279058/article/details/51035631


- groupByKey:
    value根据key单纯的聚合成一个sequence，然后进行传输。1.额外的网络传输  2.当datanode节点进行shuffle时，容易造成OOM(把需要的partitions都取过来了)。
- reduceByKey:
    在数据传输前，数据在本地会先调用reduce函数进行聚合，再进行传输
- aggregateByKey:
    调用combinerByKey,和reduceByKey一样, 需要初始值.

- combineByKey:
    更灵活，可以返回其他输出类型
    merging function
    combine function

reduceByKey,aggregateByKey,combineByKey 都比 groupByKey 好

参考：https://stackoverflow.com/questions/43364432/spark-difference-between-reducebykey-vs-groupbykey-vs-aggregatebykey-vs-combineb

-------------------------------------------------------------------------------------------------------------------------
